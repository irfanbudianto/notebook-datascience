{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "## Loan Default Prediction using Deep Neural Networks with ADASYN\n",
        "\n",
        "This notebook demonstrates a process for predicting loan defaults. Key steps include:\n",
        "1.  **Importing Libraries**: Essential packages for data manipulation, visualization, and modeling.\n",
        "2.  **Loading Dataset**: Reading the loan data.\n",
        "3.  **Exploratory Data Analysis (EDA)**: Understanding data structure, selecting features, and preparing the target variable.\n",
        "4.  **Feature Visualization**: Visualizing distributions and correlations of selected features.\n",
        "5.  **Data Preprocessing**: Splitting data, scaling features, and checking class balance.\n",
        "6.  **Handling Imbalanced Data**: Applying ADASYN to the training set to address class imbalance.\n",
        "7.  **Deep Neural Network (DNN) Modeling**: Defining, compiling, and training a DNN.\n",
        "8.  **Model Evaluation**: Assessing the model's performance using various metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v7yWorq3uJW"
      },
      "source": [
        "### 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kP5AlU-t3szc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, RobustScaler, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Visualization Setup\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.6f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dm5QklZE0JR",
        "outputId": "b5f8fe77-2dee-46aa-c001-d9c959d4d504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFEbVOj333V-"
      },
      "source": [
        "### 2. Load Dataset\n",
        "get CSV from https://www.kaggle.com/datasets/adarshsng/lending-club-loan-data-csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xcP4GTMP35DM"
      },
      "outputs": [],
      "source": [
        "# Update the path to your dataset file if necessary\n",
        "df = pd.read_parquet(\"C:/DATA/Data/loan_ORI.parquet.gzip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBPB90jQ4PFF"
      },
      "source": [
        "### 3. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV_T9gRUneqo"
      },
      "source": [
        "#### 3.1. Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1FVYxAK4T4b",
        "outputId": "d2810fe7-c9e9-4e56-fa32-ba73bb0f70b3"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLLE3xnzngHF"
      },
      "source": [
        "#### 3.2. Descriptive Statistics (Placeholder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "7s41YMASEGiK",
        "outputId": "243ece77-bdc0-4b75-af66-9ee7a70f4123"
      },
      "outputs": [],
      "source": [
        "numerical_cols_for_describe = df.select_dtypes(include=np.number).columns.tolist()\n",
        "describe_df = df[numerical_cols_for_describe].describe().T\n",
        "describe_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWQU7pU0naof"
      },
      "source": [
        "#### 3.3. Feature Selection and Initial Target Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "TgkmXXiB2MkQ",
        "outputId": "3c5c6c42-2bcd-4d5f-d885-3f98c913b765"
      },
      "outputs": [],
      "source": [
        "selected_features = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'loan_status']\n",
        "df_selected_features = df[selected_features].copy()\n",
        "\n",
        "print(\"Original Loan Status Distribution:\")\n",
        "loan_status_prs = df_selected_features['loan_status'].value_counts(normalize=True) * 100\n",
        "loan_status_count = df_selected_features['loan_status'].value_counts()\n",
        "loan_status_summary = pd.DataFrame({\n",
        "    'Count': loan_status_count,\n",
        "    'Percent (%)': loan_status_prs\n",
        "}).reset_index()\n",
        "loan_status_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEeCcjKMsA-1"
      },
      "source": [
        "#### 3.4. Target Variable Preparation for Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "2tOVmJ4M4fmM",
        "outputId": "859cfd9c-a77c-459d-e8fe-530083e25333"
      },
      "outputs": [],
      "source": [
        "df_selected = df_selected_features[df_selected_features['loan_status'].isin(['Fully Paid','Default'])].copy()\n",
        "mapping = {'Fully Paid': 0, 'Default': 1}\n",
        "df_selected['loan_status'] = df_selected['loan_status'].map(mapping)\n",
        "\n",
        "print(\"Encoded Loan Status Distribution (%):\")\n",
        "print(df_selected['loan_status'].value_counts(normalize=True) * 100)\n",
        "print(\"\\nEncoded Loan Status Distribution (count):\")\n",
        "print(df_selected['loan_status'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x='loan_status', data=df_selected)\n",
        "plt.title('Target Class Distribution (0: Fully Paid, 1: Default)')\n",
        "plt.xlabel('Loan Status (Encoded)')\n",
        "plt.ylabel('Count')\n",
        "for p in ax.patches:\n",
        "  ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
        "plt.show()\n",
        "\n",
        "target_variable = 'loan_status'\n",
        "independent_features = ['loan_amnt', 'int_rate', 'installment', 'annual_inc']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmlEDdDIz5x0"
      },
      "source": [
        "#### 3.5. Handling Missing Values in Selected Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjZCbG0X5tws",
        "outputId": "41479c67-8ea8-45ef-e20f-e0378f3af958"
      },
      "outputs": [],
      "source": [
        "print(\"Missing Values Before Handling:\")\n",
        "print(df_selected[independent_features].isnull().sum())\n",
        "\n",
        "df_selected.dropna(subset=independent_features, inplace=True)\n",
        "print(\"\\nMissing Values After Row Removal (if any):\")\n",
        "print(df_selected[independent_features].isnull().sum())\n",
        "print(f\"\\nDataset Size (After Handling Missing Values): {df_selected.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAAZ-Oz98lHU"
      },
      "source": [
        "### 4. Feature Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yIXmS8xo4Qes",
        "outputId": "4b394f4a-7a71-45ca-bcd9-fdf88a5244a6"
      },
      "outputs": [],
      "source": [
        "if not df_selected.empty and all(f in df_selected.columns for f in independent_features):\n",
        "    for col in independent_features:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.histplot(df_selected[col], kde=True, bins=30)\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    correlation_matrix = df_selected[independent_features].corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Correlation Matrix of Independent Features')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping feature visualization as df_selected is empty or features are missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eoiiYXd6KaN"
      },
      "source": [
        "### 5. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBjHunmssA-1"
      },
      "source": [
        "#### 5.1. Data Splitting & Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4THRPGy6IVx",
        "outputId": "85d78d5b-d1ae-4e6c-8ebd-656635b4c071"
      },
      "outputs": [],
      "source": [
        "X = df_selected[independent_features]\n",
        "y = df_selected[target_variable]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "loan_amnt_col = ['loan_amnt']\n",
        "other_cols = ['int_rate', 'installment', 'annual_inc']\n",
        "\n",
        "min_max_scaler_loan_amnt = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler_others = StandardScaler()\n",
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[loan_amnt_col] = min_max_scaler_loan_amnt.fit_transform(X_train[loan_amnt_col])\n",
        "X_test_scaled[loan_amnt_col] = min_max_scaler_loan_amnt.transform(X_test[loan_amnt_col])\n",
        "\n",
        "if other_cols: # Ensure other_cols is not empty\n",
        "    X_train_scaled[other_cols] = scaler_others.fit_transform(X_train[other_cols])\n",
        "    X_test_scaled[other_cols] = scaler_others.transform(X_test[other_cols])\n",
        "\n",
        "print(\"\\nDescriptive statistics of X_train_scaled:\")\n",
        "print(X_train_scaled.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8gUFxETsA-1"
      },
      "source": [
        "#### 5.2. Check Class Composition in Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqn4e0PTz7iR",
        "outputId": "d3aedcb0-5f2c-4190-96a2-4e75e77df808"
      },
      "outputs": [],
      "source": [
        "# Check composition and count of classes in 'y' after splitting\n",
        "print(\"Class composition and counts in y_train:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nClass proportions in y_train (%):\")\n",
        "print(y_train.value_counts(normalize=True) * 100)\n",
        "\n",
        "print(\"\\nClass composition and counts in y_test:\")\n",
        "print(y_test.value_counts())\n",
        "print(\"\\nClass proportions in y_test (%):\")\n",
        "print(y_test.value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3P2SV8M6SZR"
      },
      "source": [
        "### 6. Handling Imbalanced Data with ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "id": "lA_l1sG06UNw",
        "outputId": "23c171d7-d416-4ea1-c3cc-5779c1519fa1"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nClass distribution in y_train before ADASYN: \\n{y_train.value_counts(normalize=True)}\")\n",
        "\n",
        "# Save X_train_scaled and y_train before ADASYN for PCA visualization\n",
        "X_train_scaled_before_adasyn = X_train_scaled.copy()\n",
        "y_train_before_adasyn = y_train.copy()\n",
        "\n",
        "# ADASYN parameters (K=5 for n_neighbors)\n",
        "adasyn = ADASYN(random_state=420, n_neighbors=5) # n_neighbors=5 is the default\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nShape of X_train after ADASYN: {X_train_resampled.shape}\")\n",
        "print(f\"Class distribution in y_train after ADASYN: \\n{pd.Series(y_train_resampled).value_counts(normalize=True)}\")\n",
        "\n",
        "# --- Visualize ADASYN Results with PCA ---\n",
        "print(\"\\nVisualizing ADASYN results with PCA (2 components)...\")\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Apply PCA to training data BEFORE ADASYN\n",
        "X_train_pca_before = pca.fit_transform(X_train_scaled_before_adasyn)\n",
        "\n",
        "# Apply PCA to training data AFTER ADASYN\n",
        "if isinstance(X_train_resampled, np.ndarray):\n",
        "    X_train_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train_scaled.columns)\n",
        "else:\n",
        "    X_train_resampled_df = X_train_resampled\n",
        "\n",
        "X_train_pca_after = pca.transform(X_train_resampled_df) # Use transform as PCA is already fitted\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot before ADASYN\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train_pca_before[y_train_before_adasyn == 0, 0], X_train_pca_before[y_train_before_adasyn == 0, 1], label='Majority (Class 0 - Fully Paid)', alpha=0.5, s=10)\n",
        "plt.scatter(X_train_pca_before[y_train_before_adasyn == 1, 0], X_train_pca_before[y_train_before_adasyn == 1, 1], label='Minority (Class 1 - Default)', alpha=0.7, s=15, c='red')\n",
        "plt.title('Training Data Before ADASYN (PCA 2D)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend()\n",
        "\n",
        "# Plot after ADASYN\n",
        "plt.subplot(1, 2, 2)\n",
        "# Separate original and synthetic data for plotting\n",
        "num_original_minority_before = (y_train_before_adasyn == 1).sum()\n",
        "num_original_majority_before = (y_train_before_adasyn == 0).sum()\n",
        "\n",
        "plt.scatter(X_train_pca_after[y_train_resampled == 0, 0], X_train_pca_after[y_train_resampled == 0, 1], label='Majority (Class 0 - Fully Paid)', alpha=0.3, s=10)\n",
        "plt.scatter(X_train_pca_after[y_train_resampled == 1, 0], X_train_pca_after[y_train_resampled == 1, 1], label='Minority + Synthetic (Class 1 - Default)', alpha=0.5, s=15, c='red')\n",
        "\n",
        "plt.title('Training Data After ADASYN (PCA 2D)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v6GQxC-6nyL"
      },
      "source": [
        "### 7. Deep Neural Network (DNN) Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qydrbo6esA-1"
      },
      "source": [
        "#### 7.1. Define DNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "enuTm2du6pOZ",
        "outputId": "57b7cf77-e7b5-4c24-c429-15b935c0e978"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train_resampled.shape[1],)), # Input shape based on number of features\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3), # Adding Dropout for regularization\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Optimizer: Adam, Loss: binary_crossentropy (for binary classification)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Callback for Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8X9BRadsA-1"
      },
      "source": [
        "#### 7.2. Train the DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRZz40rk6xMF",
        "outputId": "235b369f-6397-417f-9f15-f53ba83a0e18"
      },
      "outputs": [],
      "source": [
        "print(\"\\nStarting DNN model training...\")\n",
        "history = model.fit(\n",
        "    X_train_resampled, y_train_resampled, # Use ADASYN-resampled data\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2, # Use 20% of training data for validation\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Model training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9VYwZumsA-2"
      },
      "source": [
        "#### 7.3. Plot Training History\n",
        "Visualize the model's training and validation accuracy and loss over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "CjPlLY3w7NCX",
        "outputId": "6f2af011-5264-476c-b515-dba891be1806"
      },
      "outputs": [],
      "source": [
        "if history is not None:\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    if 'val_accuracy' in history.history:\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training history is not available for plotting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBXTM6rf7P-2"
      },
      "source": [
        "### 8. Model Evaluation\n",
        "Evaluate the trained model on the (unseen) test set. Calculate and display accuracy, precision, recall, specificity, confusion matrix, and classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(X_test_scaled, y_test, verbose=1)\n",
        "# print(f\"\\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "Ch1N35HY2Ujb",
        "outputId": "0b2e16cc-407f-429c-e45d-3ce7117f5df1"
      },
      "outputs": [],
      "source": [
        "# Predictions on the test set\n",
        "y_pred_proba = model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int) # Threshold 0.5 for binary classification\n",
        "\n",
        "# Evaluation Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=1) # Precision for the 'Default' class (1)\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)       # Recall for the 'Default' class (1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Ensure cm has 4 elements before unpacking (tn, fp, fn, tp)\n",
        "if cm.size == 4:\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "else: # Handle cases where the confusion matrix might not be 2x2 (e.g., if one class has no predictions)\n",
        "    print(\"Warning: Confusion matrix is not 2x2. Specificity might not be directly calculable or meaningful.\")\n",
        "    print(f\"Confusion Matrix shape: {cm.shape}\")\n",
        "    print(f\"y_test unique values and counts: {np.unique(y_test, return_counts=True)}\")\n",
        "    print(f\"y_pred unique values and counts: {np.unique(y_pred, return_counts=True)}\")\n",
        "    # Set specificity to NaN or a default value if it cannot be calculated\n",
        "    specificity = np.nan\n",
        "    # If only one class is predicted, tn or tp might be the total count for that class, and the other pair (fp, fn) would be zero.\n",
        "    # For example, if only class 0 is predicted: tn = sum(y_test==0), fp = 0, fn = sum(y_test==1), tp = 0.\n",
        "    # Or if only class 1 is predicted: tn = 0, fp = sum(y_test==0), fn = 0, tp = sum(y_test==1).\n",
        "    if len(np.unique(y_pred)) == 1:\n",
        "        if np.unique(y_pred)[0] == 0: # All predicted as class 0\n",
        "            tn = cm[0,0] if cm.shape == (1,1) or (cm.shape[0] > 0 and cm.shape[1] > 0) else np.sum(y_test==0) # approximation\n",
        "            fp = 0\n",
        "            specificity = 1.0 if tn > 0 else 0 # Or based on actual tn/(tn+fp)\n",
        "        elif np.unique(y_pred)[0] == 1: # All predicted as class 1\n",
        "            tn = 0\n",
        "            fp = np.sum(y_test==0) # approximation\n",
        "            specificity = 0.0\n",
        "\n",
        "print(\"\\n--- MODEL EVALUATION RESULTS ---\")\n",
        "print(f\"Accuracy           : {accuracy:.4f} (Paper target: 0.941)\")\n",
        "print(f\"Precision (Default): {precision:.4f} (Paper target: 0.972 for Default class)\")\n",
        "print(f\"Recall (Sensitivity, Default): {recall:.4f} (Paper target: 0.960 for Default class)\")\n",
        "print(f\"Specificity (Fully Paid): {specificity:.4f} (Paper target: 0.823 for Fully Paid class)\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Fully Paid (0)', 'Predicted Default (1)'],\n",
        "            yticklabels=['Actual Fully Paid (0)', 'Actual Default (1)'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Fully Paid (0)', 'Default (1)']))\n",
        "\n",
        "print(\"\\nNOTES:\")\n",
        "print(\"1. Results can vary depending on the specific dataset split, preprocessing, and random weight initialization in the DNN.\")\n",
        "print(\"2. Ensure the dataset path ('/content/drive/MyDrive/Data/loan_ORI.parquet.gzip') is correct.\")\n",
        "print(\"3. The reference paper might use a different version or source of the Lending Club dataset (e.g., 2007-2015), which could lead to different metrics.\")\n",
        "print(\"4. Hyperparameters such as the number of units in hidden layers, dropout rate, learning rate, etc., can be further tuned for optimization.\")\n",
        "print(\"5. Feature scaling was applied: 'loan_amnt' scaled to [-1,1] with MinMaxScaler, other specified features with RobustScaler.\")\n",
        "print(\"6. PCA visualization was added to show the effect of ADASYN on the training data distribution.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
